{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is to learn how to implement a RNN-LSTM model in keras \n",
    "#to predict the next digit in the sequence of the numbers. \n",
    "\n",
    "# Importn the libraries needed. \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[[(i+j)/100] for i in range(5)]for j in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = [(i+5)/100 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us convert them  into numpy arrays. \n",
    "\n",
    "data = np.array(data, dtype = float)\n",
    "target = np.array(target, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data,target,test_size = 0.2,\n",
    "                                                random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM((1),batch_input_shape = (None,5,1),return_sequences = True))\n",
    "model.add(LSTM(1,return_sequences = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compipling the model. \n",
    "\n",
    "model.compile(loss = 'mean_absolute_error',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 5, 1)              12        \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/400\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0256 - acc: 0.9750 - val_loss: 0.1508 - val_acc: 0.8500\n",
      "Epoch 2/400\n",
      "80/80 [==============================] - 0s 254us/step - loss: 0.0257 - acc: 0.9750 - val_loss: 0.1507 - val_acc: 0.8500\n",
      "Epoch 3/400\n",
      "80/80 [==============================] - 0s 283us/step - loss: 0.0254 - acc: 0.9750 - val_loss: 0.1507 - val_acc: 0.8500\n",
      "Epoch 4/400\n",
      "80/80 [==============================] - 0s 345us/step - loss: 0.0254 - acc: 0.9750 - val_loss: 0.1506 - val_acc: 0.8500\n",
      "Epoch 5/400\n",
      "80/80 [==============================] - 0s 266us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 6/400\n",
      "80/80 [==============================] - 0s 416us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1506 - val_acc: 0.8500\n",
      "Epoch 7/400\n",
      "80/80 [==============================] - 0s 323us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 8/400\n",
      "80/80 [==============================] - 0s 285us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 9/400\n",
      "80/80 [==============================] - 0s 306us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 10/400\n",
      "80/80 [==============================] - 0s 407us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 11/400\n",
      "80/80 [==============================] - 0s 351us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 12/400\n",
      "80/80 [==============================] - 0s 318us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 13/400\n",
      "80/80 [==============================] - 0s 351us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 14/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 15/400\n",
      "80/80 [==============================] - 0s 410us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 16/400\n",
      "80/80 [==============================] - 0s 443us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1505 - val_acc: 0.8500\n",
      "Epoch 17/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 18/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 19/400\n",
      "80/80 [==============================] - 0s 388us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 20/400\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 21/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 22/400\n",
      "80/80 [==============================] - 0s 298us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 23/400\n",
      "80/80 [==============================] - 0s 360us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 24/400\n",
      "80/80 [==============================] - 0s 321us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 25/400\n",
      "80/80 [==============================] - 0s 330us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 26/400\n",
      "80/80 [==============================] - 0s 313us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 27/400\n",
      "80/80 [==============================] - 0s 339us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 28/400\n",
      "80/80 [==============================] - 0s 346us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 29/400\n",
      "80/80 [==============================] - 0s 405us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 30/400\n",
      "80/80 [==============================] - 0s 308us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 31/400\n",
      "80/80 [==============================] - 0s 462us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1504 - val_acc: 0.8500\n",
      "Epoch 32/400\n",
      "80/80 [==============================] - 0s 269us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 33/400\n",
      "80/80 [==============================] - 0s 420us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 34/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 35/400\n",
      "80/80 [==============================] - 0s 348us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 36/400\n",
      "80/80 [==============================] - 0s 387us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 37/400\n",
      "80/80 [==============================] - 0s 271us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 38/400\n",
      "80/80 [==============================] - 0s 376us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 39/400\n",
      "80/80 [==============================] - 0s 276us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 40/400\n",
      "80/80 [==============================] - 0s 314us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 41/400\n",
      "80/80 [==============================] - 0s 431us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 42/400\n",
      "80/80 [==============================] - 0s 373us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 43/400\n",
      "80/80 [==============================] - 0s 491us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 44/400\n",
      "80/80 [==============================] - 0s 255us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 45/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 46/400\n",
      "80/80 [==============================] - 0s 355us/step - loss: 0.0253 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 47/400\n",
      "80/80 [==============================] - 0s 339us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1503 - val_acc: 0.8500\n",
      "Epoch 48/400\n",
      "80/80 [==============================] - 0s 386us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 49/400\n",
      "80/80 [==============================] - 0s 308us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 50/400\n",
      "80/80 [==============================] - 0s 328us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 51/400\n",
      "80/80 [==============================] - 0s 291us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 52/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 53/400\n",
      "80/80 [==============================] - 0s 354us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 54/400\n",
      "80/80 [==============================] - 0s 425us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 55/400\n",
      "80/80 [==============================] - 0s 301us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 56/400\n",
      "80/80 [==============================] - 0s 390us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 57/400\n",
      "80/80 [==============================] - 0s 369us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 58/400\n",
      "80/80 [==============================] - 0s 305us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 59/400\n",
      "80/80 [==============================] - 0s 370us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 60/400\n",
      "80/80 [==============================] - 0s 379us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 61/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/400\n",
      "80/80 [==============================] - 0s 340us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 63/400\n",
      "80/80 [==============================] - 0s 292us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 64/400\n",
      "80/80 [==============================] - 0s 269us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 65/400\n",
      "80/80 [==============================] - 0s 244us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 66/400\n",
      "80/80 [==============================] - 0s 259us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 67/400\n",
      "80/80 [==============================] - 0s 241us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 68/400\n",
      "80/80 [==============================] - 0s 272us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 69/400\n",
      "80/80 [==============================] - 0s 272us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 70/400\n",
      "80/80 [==============================] - 0s 231us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 71/400\n",
      "80/80 [==============================] - 0s 272us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 72/400\n",
      "80/80 [==============================] - 0s 264us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 73/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 74/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 75/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 76/400\n",
      "80/80 [==============================] - 0s 265us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 77/400\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 78/400\n",
      "80/80 [==============================] - 0s 291us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1502 - val_acc: 0.8500\n",
      "Epoch 79/400\n",
      "80/80 [==============================] - 0s 259us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 80/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0252 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 81/400\n",
      "80/80 [==============================] - 0s 284us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 82/400\n",
      "80/80 [==============================] - 0s 263us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 83/400\n",
      "80/80 [==============================] - 0s 260us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 84/400\n",
      "80/80 [==============================] - 0s 242us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 85/400\n",
      "80/80 [==============================] - 0s 259us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 86/400\n",
      "80/80 [==============================] - 0s 270us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 87/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 88/400\n",
      "80/80 [==============================] - 0s 330us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 89/400\n",
      "80/80 [==============================] - 0s 305us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 90/400\n",
      "80/80 [==============================] - 0s 393us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 91/400\n",
      "80/80 [==============================] - 0s 319us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 92/400\n",
      "80/80 [==============================] - 0s 303us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 93/400\n",
      "80/80 [==============================] - 0s 332us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 94/400\n",
      "80/80 [==============================] - 0s 383us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 95/400\n",
      "80/80 [==============================] - 0s 365us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 96/400\n",
      "80/80 [==============================] - 0s 398us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 97/400\n",
      "80/80 [==============================] - 0s 371us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 98/400\n",
      "80/80 [==============================] - 0s 331us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 99/400\n",
      "80/80 [==============================] - 0s 372us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 100/400\n",
      "80/80 [==============================] - 0s 360us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 101/400\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 102/400\n",
      "80/80 [==============================] - 0s 336us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 103/400\n",
      "80/80 [==============================] - 0s 298us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 104/400\n",
      "80/80 [==============================] - 0s 313us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 105/400\n",
      "80/80 [==============================] - 0s 333us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 106/400\n",
      "80/80 [==============================] - 0s 338us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 107/400\n",
      "80/80 [==============================] - 0s 368us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1501 - val_acc: 0.8500\n",
      "Epoch 108/400\n",
      "80/80 [==============================] - 0s 328us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 109/400\n",
      "80/80 [==============================] - 0s 342us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 110/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 111/400\n",
      "80/80 [==============================] - 0s 371us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 112/400\n",
      "80/80 [==============================] - 0s 341us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 113/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 114/400\n",
      "80/80 [==============================] - 0s 333us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 115/400\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 116/400\n",
      "80/80 [==============================] - 0s 346us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 117/400\n",
      "80/80 [==============================] - 0s 380us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 118/400\n",
      "80/80 [==============================] - 0s 369us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 119/400\n",
      "80/80 [==============================] - 0s 321us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 120/400\n",
      "80/80 [==============================] - 0s 333us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 121/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 122/400\n",
      "80/80 [==============================] - 0s 477us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/400\n",
      "80/80 [==============================] - 0s 352us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 124/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 125/400\n",
      "80/80 [==============================] - 0s 251us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 126/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.0458e-04 - acc: 1.000 - 0s 289us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 127/400\n",
      "80/80 [==============================] - 0s 325us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 128/400\n",
      "80/80 [==============================] - 0s 273us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 129/400\n",
      "80/80 [==============================] - 0s 291us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1500 - val_acc: 0.8500\n",
      "Epoch 130/400\n",
      "80/80 [==============================] - 0s 259us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 131/400\n",
      "80/80 [==============================] - 0s 279us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 132/400\n",
      "80/80 [==============================] - 0s 304us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 133/400\n",
      "80/80 [==============================] - 0s 370us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 134/400\n",
      "80/80 [==============================] - 0s 278us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 135/400\n",
      "80/80 [==============================] - 0s 288us/step - loss: 0.0251 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 136/400\n",
      "80/80 [==============================] - 0s 318us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 137/400\n",
      "80/80 [==============================] - 0s 377us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 138/400\n",
      "80/80 [==============================] - 0s 251us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 139/400\n",
      "80/80 [==============================] - 0s 304us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 140/400\n",
      "80/80 [==============================] - 0s 292us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 141/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1499 - val_acc: 0.8500\n",
      "Epoch 142/400\n",
      "80/80 [==============================] - 0s 319us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 143/400\n",
      "80/80 [==============================] - 0s 271us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 144/400\n",
      "80/80 [==============================] - 0s 319us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 145/400\n",
      "80/80 [==============================] - 0s 398us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 146/400\n",
      "80/80 [==============================] - 0s 283us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 147/400\n",
      "80/80 [==============================] - 0s 309us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 148/400\n",
      "80/80 [==============================] - 0s 310us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 149/400\n",
      "80/80 [==============================] - 0s 380us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 150/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 151/400\n",
      "80/80 [==============================] - 0s 394us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 152/400\n",
      "80/80 [==============================] - 0s 293us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 153/400\n",
      "80/80 [==============================] - 0s 376us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 154/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 155/400\n",
      "80/80 [==============================] - 0s 303us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 156/400\n",
      "80/80 [==============================] - 0s 362us/step - loss: 0.0250 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 157/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 158/400\n",
      "80/80 [==============================] - 0s 276us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.8500\n",
      "Epoch 159/400\n",
      "80/80 [==============================] - 0s 323us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 160/400\n",
      "80/80 [==============================] - 0s 418us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 161/400\n",
      "80/80 [==============================] - 0s 310us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 162/400\n",
      "80/80 [==============================] - 0s 353us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 163/400\n",
      "80/80 [==============================] - 0s 388us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 164/400\n",
      "80/80 [==============================] - 0s 392us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 165/400\n",
      "80/80 [==============================] - 0s 483us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 166/400\n",
      "80/80 [==============================] - 0s 308us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 167/400\n",
      "80/80 [==============================] - 0s 389us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 168/400\n",
      "80/80 [==============================] - 0s 344us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 169/400\n",
      "80/80 [==============================] - 0s 413us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 170/400\n",
      "80/80 [==============================] - 0s 365us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 171/400\n",
      "80/80 [==============================] - 0s 333us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 172/400\n",
      "80/80 [==============================] - 0s 379us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.8500\n",
      "Epoch 173/400\n",
      "80/80 [==============================] - 0s 348us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 174/400\n",
      "80/80 [==============================] - 0s 293us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 175/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0249 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 176/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 177/400\n",
      "80/80 [==============================] - 0s 319us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1496 - val_acc: 0.8500\n",
      "Epoch 178/400\n",
      "80/80 [==============================] - 0s 332us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 179/400\n",
      "80/80 [==============================] - 0s 446us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 180/400\n",
      "80/80 [==============================] - 0s 351us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 181/400\n",
      "80/80 [==============================] - 0s 387us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 182/400\n",
      "80/80 [==============================] - 0s 346us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1495 - val_acc: 0.8500\n",
      "Epoch 183/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 422us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 184/400\n",
      "80/80 [==============================] - 0s 330us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 185/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 186/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 187/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 188/400\n",
      "80/80 [==============================] - 0s 292us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1494 - val_acc: 0.8500\n",
      "Epoch 189/400\n",
      "80/80 [==============================] - 0s 316us/step - loss: 0.0248 - acc: 0.9750 - val_loss: 0.1493 - val_acc: 0.8500\n",
      "Epoch 190/400\n",
      "80/80 [==============================] - 0s 338us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1493 - val_acc: 0.8500\n",
      "Epoch 191/400\n",
      "80/80 [==============================] - 0s 267us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1492 - val_acc: 0.8500\n",
      "Epoch 192/400\n",
      "80/80 [==============================] - 0s 291us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1493 - val_acc: 0.8500\n",
      "Epoch 193/400\n",
      "80/80 [==============================] - 0s 369us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1493 - val_acc: 0.8500\n",
      "Epoch 194/400\n",
      "80/80 [==============================] - 0s 345us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1492 - val_acc: 0.8500\n",
      "Epoch 195/400\n",
      "80/80 [==============================] - 0s 264us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1492 - val_acc: 0.8500\n",
      "Epoch 196/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0247 - acc: 0.9750 - val_loss: 0.1491 - val_acc: 0.8500\n",
      "Epoch 197/400\n",
      "80/80 [==============================] - 0s 382us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1492 - val_acc: 0.8500\n",
      "Epoch 198/400\n",
      "80/80 [==============================] - 0s 293us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1491 - val_acc: 0.8500\n",
      "Epoch 199/400\n",
      "80/80 [==============================] - 0s 240us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1491 - val_acc: 0.8500\n",
      "Epoch 200/400\n",
      "80/80 [==============================] - 0s 323us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1490 - val_acc: 0.8500\n",
      "Epoch 201/400\n",
      "80/80 [==============================] - 0s 372us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1491 - val_acc: 0.8500\n",
      "Epoch 202/400\n",
      "80/80 [==============================] - 0s 295us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1490 - val_acc: 0.8500\n",
      "Epoch 203/400\n",
      "80/80 [==============================] - 0s 265us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1489 - val_acc: 0.8500\n",
      "Epoch 204/400\n",
      "80/80 [==============================] - 0s 303us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1491 - val_acc: 0.8500\n",
      "Epoch 205/400\n",
      "80/80 [==============================] - 0s 359us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1489 - val_acc: 0.8500\n",
      "Epoch 206/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1490 - val_acc: 0.8500\n",
      "Epoch 207/400\n",
      "80/80 [==============================] - 0s 397us/step - loss: 0.0246 - acc: 0.9750 - val_loss: 0.1489 - val_acc: 0.8500\n",
      "Epoch 208/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1489 - val_acc: 0.8500\n",
      "Epoch 209/400\n",
      "80/80 [==============================] - 0s 305us/step - loss: 0.0244 - acc: 0.9750 - val_loss: 0.1489 - val_acc: 0.8500\n",
      "Epoch 210/400\n",
      "80/80 [==============================] - 0s 320us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1488 - val_acc: 0.8500\n",
      "Epoch 211/400\n",
      "80/80 [==============================] - 0s 362us/step - loss: 0.0245 - acc: 0.9750 - val_loss: 0.1488 - val_acc: 0.8500\n",
      "Epoch 212/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0244 - acc: 0.9750 - val_loss: 0.1487 - val_acc: 0.8500\n",
      "Epoch 213/400\n",
      "80/80 [==============================] - 0s 375us/step - loss: 0.0243 - acc: 0.9750 - val_loss: 0.1486 - val_acc: 0.8500\n",
      "Epoch 214/400\n",
      "80/80 [==============================] - 0s 341us/step - loss: 0.0243 - acc: 0.9750 - val_loss: 0.1486 - val_acc: 0.8500\n",
      "Epoch 215/400\n",
      "80/80 [==============================] - 0s 403us/step - loss: 0.0243 - acc: 0.9750 - val_loss: 0.1487 - val_acc: 0.8500\n",
      "Epoch 216/400\n",
      "80/80 [==============================] - 0s 329us/step - loss: 0.0243 - acc: 0.9750 - val_loss: 0.1485 - val_acc: 0.8500\n",
      "Epoch 217/400\n",
      "80/80 [==============================] - 0s 280us/step - loss: 0.0242 - acc: 0.9750 - val_loss: 0.1485 - val_acc: 0.8500\n",
      "Epoch 218/400\n",
      "80/80 [==============================] - 0s 310us/step - loss: 0.0242 - acc: 0.9750 - val_loss: 0.1484 - val_acc: 0.8500\n",
      "Epoch 219/400\n",
      "80/80 [==============================] - 0s 322us/step - loss: 0.0242 - acc: 0.9750 - val_loss: 0.1484 - val_acc: 0.8500\n",
      "Epoch 220/400\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.0242 - acc: 0.9750 - val_loss: 0.1483 - val_acc: 0.8500\n",
      "Epoch 221/400\n",
      "80/80 [==============================] - 0s 440us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1482 - val_acc: 0.8500\n",
      "Epoch 222/400\n",
      "80/80 [==============================] - 0s 404us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1483 - val_acc: 0.8500\n",
      "Epoch 223/400\n",
      "80/80 [==============================] - 0s 341us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1483 - val_acc: 0.8500\n",
      "Epoch 224/400\n",
      "80/80 [==============================] - 0s 342us/step - loss: 0.0242 - acc: 0.9750 - val_loss: 0.1481 - val_acc: 0.8500\n",
      "Epoch 225/400\n",
      "80/80 [==============================] - 0s 356us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1483 - val_acc: 0.8500\n",
      "Epoch 226/400\n",
      "80/80 [==============================] - 0s 367us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1480 - val_acc: 0.8500\n",
      "Epoch 227/400\n",
      "80/80 [==============================] - 0s 385us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1480 - val_acc: 0.8500\n",
      "Epoch 228/400\n",
      "80/80 [==============================] - 0s 407us/step - loss: 0.0239 - acc: 0.9750 - val_loss: 0.1481 - val_acc: 0.8500\n",
      "Epoch 229/400\n",
      "80/80 [==============================] - 0s 302us/step - loss: 0.0241 - acc: 0.9750 - val_loss: 0.1477 - val_acc: 0.8500\n",
      "Epoch 230/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0239 - acc: 0.9750 - val_loss: 0.1479 - val_acc: 0.8500\n",
      "Epoch 231/400\n",
      "80/80 [==============================] - 0s 298us/step - loss: 0.0239 - acc: 0.9750 - val_loss: 0.1477 - val_acc: 0.8500\n",
      "Epoch 232/400\n",
      "80/80 [==============================] - 0s 363us/step - loss: 0.0238 - acc: 0.9750 - val_loss: 0.1476 - val_acc: 0.8500\n",
      "Epoch 233/400\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.0236 - acc: 0.9750 - val_loss: 0.1474 - val_acc: 0.8500\n",
      "Epoch 234/400\n",
      "80/80 [==============================] - 0s 405us/step - loss: 0.0235 - acc: 0.9750 - val_loss: 0.1473 - val_acc: 0.8500\n",
      "Epoch 235/400\n",
      "80/80 [==============================] - 0s 382us/step - loss: 0.0235 - acc: 0.9750 - val_loss: 0.1472 - val_acc: 0.8500\n",
      "Epoch 236/400\n",
      "80/80 [==============================] - 0s 378us/step - loss: 0.0234 - acc: 0.9750 - val_loss: 0.1471 - val_acc: 0.8500\n",
      "Epoch 237/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0233 - acc: 0.9750 - val_loss: 0.1471 - val_acc: 0.8500\n",
      "Epoch 238/400\n",
      "80/80 [==============================] - 0s 331us/step - loss: 0.0234 - acc: 0.9750 - val_loss: 0.1471 - val_acc: 0.8500\n",
      "Epoch 239/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0234 - acc: 0.9750 - val_loss: 0.1468 - val_acc: 0.8500\n",
      "Epoch 240/400\n",
      "80/80 [==============================] - 0s 394us/step - loss: 0.0232 - acc: 0.9750 - val_loss: 0.1467 - val_acc: 0.8500\n",
      "Epoch 241/400\n",
      "80/80 [==============================] - 0s 387us/step - loss: 0.0231 - acc: 0.9750 - val_loss: 0.1466 - val_acc: 0.8500\n",
      "Epoch 242/400\n",
      "80/80 [==============================] - 0s 356us/step - loss: 0.0230 - acc: 0.9750 - val_loss: 0.1465 - val_acc: 0.8500\n",
      "Epoch 243/400\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.0229 - acc: 0.9750 - val_loss: 0.1464 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/400\n",
      "80/80 [==============================] - 0s 253us/step - loss: 0.0229 - acc: 0.9750 - val_loss: 0.1464 - val_acc: 0.8500\n",
      "Epoch 245/400\n",
      "80/80 [==============================] - 0s 401us/step - loss: 0.0229 - acc: 0.9750 - val_loss: 0.1463 - val_acc: 0.8500\n",
      "Epoch 246/400\n",
      "80/80 [==============================] - 0s 302us/step - loss: 0.0228 - acc: 0.9750 - val_loss: 0.1460 - val_acc: 0.8500\n",
      "Epoch 247/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0286 - acc: 0.968 - 0s 277us/step - loss: 0.0226 - acc: 0.9750 - val_loss: 0.1460 - val_acc: 0.8500\n",
      "Epoch 248/400\n",
      "80/80 [==============================] - 0s 306us/step - loss: 0.0226 - acc: 0.9750 - val_loss: 0.1459 - val_acc: 0.8500\n",
      "Epoch 249/400\n",
      "80/80 [==============================] - 0s 393us/step - loss: 0.0227 - acc: 0.9750 - val_loss: 0.1456 - val_acc: 0.8500\n",
      "Epoch 250/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0225 - acc: 0.9750 - val_loss: 0.1458 - val_acc: 0.8500\n",
      "Epoch 251/400\n",
      "80/80 [==============================] - 0s 410us/step - loss: 0.0224 - acc: 0.9750 - val_loss: 0.1456 - val_acc: 0.8500\n",
      "Epoch 252/400\n",
      "80/80 [==============================] - 0s 259us/step - loss: 0.0224 - acc: 0.9750 - val_loss: 0.1452 - val_acc: 0.8500\n",
      "Epoch 253/400\n",
      "80/80 [==============================] - 0s 439us/step - loss: 0.0221 - acc: 0.9750 - val_loss: 0.1450 - val_acc: 0.8500\n",
      "Epoch 254/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0219 - acc: 0.9750 - val_loss: 0.1450 - val_acc: 0.8500\n",
      "Epoch 255/400\n",
      "80/80 [==============================] - 0s 340us/step - loss: 0.0220 - acc: 0.9750 - val_loss: 0.1446 - val_acc: 0.8500\n",
      "Epoch 256/400\n",
      "80/80 [==============================] - 0s 355us/step - loss: 0.0216 - acc: 0.9750 - val_loss: 0.1447 - val_acc: 0.8500\n",
      "Epoch 257/400\n",
      "80/80 [==============================] - 0s 323us/step - loss: 0.0217 - acc: 0.9750 - val_loss: 0.1443 - val_acc: 0.8500\n",
      "Epoch 258/400\n",
      "80/80 [==============================] - 0s 355us/step - loss: 0.0215 - acc: 0.9750 - val_loss: 0.1443 - val_acc: 0.8500\n",
      "Epoch 259/400\n",
      "80/80 [==============================] - 0s 290us/step - loss: 0.0214 - acc: 0.9750 - val_loss: 0.1440 - val_acc: 0.8500\n",
      "Epoch 260/400\n",
      "80/80 [==============================] - 0s 314us/step - loss: 0.0213 - acc: 0.9750 - val_loss: 0.1437 - val_acc: 0.8500\n",
      "Epoch 261/400\n",
      "80/80 [==============================] - 0s 295us/step - loss: 0.0210 - acc: 0.9750 - val_loss: 0.1435 - val_acc: 0.8500\n",
      "Epoch 262/400\n",
      "80/80 [==============================] - 0s 359us/step - loss: 0.0209 - acc: 0.9750 - val_loss: 0.1432 - val_acc: 0.8500\n",
      "Epoch 263/400\n",
      "80/80 [==============================] - 0s 345us/step - loss: 0.0206 - acc: 0.9750 - val_loss: 0.1431 - val_acc: 0.8500\n",
      "Epoch 264/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0205 - acc: 0.9750 - val_loss: 0.1429 - val_acc: 0.8500\n",
      "Epoch 265/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0246 - acc: 0.968 - 0s 423us/step - loss: 0.0203 - acc: 0.9750 - val_loss: 0.1426 - val_acc: 0.8500\n",
      "Epoch 266/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0202 - acc: 0.9750 - val_loss: 0.1423 - val_acc: 0.8500\n",
      "Epoch 267/400\n",
      "80/80 [==============================] - 0s 458us/step - loss: 0.0199 - acc: 0.9750 - val_loss: 0.1421 - val_acc: 0.8500\n",
      "Epoch 268/400\n",
      "80/80 [==============================] - 0s 447us/step - loss: 0.0197 - acc: 0.9750 - val_loss: 0.1419 - val_acc: 0.8500\n",
      "Epoch 269/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0196 - acc: 0.9750 - val_loss: 0.1420 - val_acc: 0.8500\n",
      "Epoch 270/400\n",
      "80/80 [==============================] - 0s 380us/step - loss: 0.0198 - acc: 0.9750 - val_loss: 0.1414 - val_acc: 0.8500\n",
      "Epoch 271/400\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.0194 - acc: 0.9750 - val_loss: 0.1415 - val_acc: 0.8500\n",
      "Epoch 272/400\n",
      "80/80 [==============================] - 0s 254us/step - loss: 0.0193 - acc: 0.9750 - val_loss: 0.1409 - val_acc: 0.8500\n",
      "Epoch 273/400\n",
      "80/80 [==============================] - 0s 383us/step - loss: 0.0188 - acc: 0.9750 - val_loss: 0.1407 - val_acc: 0.8500\n",
      "Epoch 274/400\n",
      "80/80 [==============================] - 0s 363us/step - loss: 0.0186 - acc: 0.9750 - val_loss: 0.1405 - val_acc: 0.8500\n",
      "Epoch 275/400\n",
      "80/80 [==============================] - 0s 403us/step - loss: 0.0185 - acc: 0.9750 - val_loss: 0.1400 - val_acc: 0.8500\n",
      "Epoch 276/400\n",
      "80/80 [==============================] - 0s 352us/step - loss: 0.0180 - acc: 0.9750 - val_loss: 0.1400 - val_acc: 0.8500\n",
      "Epoch 277/400\n",
      "80/80 [==============================] - 0s 323us/step - loss: 0.0180 - acc: 0.9750 - val_loss: 0.1393 - val_acc: 0.8500\n",
      "Epoch 278/400\n",
      "80/80 [==============================] - 0s 369us/step - loss: 0.0175 - acc: 0.9750 - val_loss: 0.1393 - val_acc: 0.8500\n",
      "Epoch 279/400\n",
      "80/80 [==============================] - 0s 354us/step - loss: 0.0174 - acc: 0.9750 - val_loss: 0.1386 - val_acc: 0.8500\n",
      "Epoch 280/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0170 - acc: 0.9750 - val_loss: 0.1384 - val_acc: 0.8500\n",
      "Epoch 281/400\n",
      "80/80 [==============================] - 0s 514us/step - loss: 0.0168 - acc: 0.9750 - val_loss: 0.1379 - val_acc: 0.8500\n",
      "Epoch 282/400\n",
      "80/80 [==============================] - 0s 291us/step - loss: 0.0165 - acc: 0.9750 - val_loss: 0.1375 - val_acc: 0.8500\n",
      "Epoch 283/400\n",
      "80/80 [==============================] - 0s 375us/step - loss: 0.0162 - acc: 0.9750 - val_loss: 0.1374 - val_acc: 0.8500\n",
      "Epoch 284/400\n",
      "80/80 [==============================] - 0s 460us/step - loss: 0.0161 - acc: 0.9750 - val_loss: 0.1373 - val_acc: 0.8500\n",
      "Epoch 285/400\n",
      "80/80 [==============================] - 0s 391us/step - loss: 0.0163 - acc: 0.9750 - val_loss: 0.1369 - val_acc: 0.8500\n",
      "Epoch 286/400\n",
      "80/80 [==============================] - 0s 410us/step - loss: 0.0154 - acc: 0.9750 - val_loss: 0.1360 - val_acc: 0.8500\n",
      "Epoch 287/400\n",
      "80/80 [==============================] - 0s 366us/step - loss: 0.0150 - acc: 0.9750 - val_loss: 0.1356 - val_acc: 0.8500\n",
      "Epoch 288/400\n",
      "80/80 [==============================] - 0s 345us/step - loss: 0.0147 - acc: 0.9750 - val_loss: 0.1353 - val_acc: 0.8500\n",
      "Epoch 289/400\n",
      "80/80 [==============================] - 0s 395us/step - loss: 0.0145 - acc: 0.9750 - val_loss: 0.1350 - val_acc: 0.8500\n",
      "Epoch 290/400\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.0142 - acc: 0.9875 - val_loss: 0.1344 - val_acc: 0.8500\n",
      "Epoch 291/400\n",
      "80/80 [==============================] - 0s 231us/step - loss: 0.0139 - acc: 0.9875 - val_loss: 0.1341 - val_acc: 0.8500\n",
      "Epoch 292/400\n",
      "80/80 [==============================] - 0s 394us/step - loss: 0.0135 - acc: 0.9875 - val_loss: 0.1337 - val_acc: 0.8500\n",
      "Epoch 293/400\n",
      "80/80 [==============================] - 0s 350us/step - loss: 0.0133 - acc: 0.9875 - val_loss: 0.1331 - val_acc: 0.8500\n",
      "Epoch 294/400\n",
      "80/80 [==============================] - 0s 397us/step - loss: 0.0128 - acc: 0.9875 - val_loss: 0.1329 - val_acc: 0.8500\n",
      "Epoch 295/400\n",
      "80/80 [==============================] - 0s 383us/step - loss: 0.0127 - acc: 0.9875 - val_loss: 0.1323 - val_acc: 0.8500\n",
      "Epoch 296/400\n",
      "80/80 [==============================] - 0s 402us/step - loss: 0.0126 - acc: 0.9875 - val_loss: 0.1325 - val_acc: 0.8500\n",
      "Epoch 297/400\n",
      "80/80 [==============================] - 0s 358us/step - loss: 0.0121 - acc: 0.9875 - val_loss: 0.1314 - val_acc: 0.8500\n",
      "Epoch 298/400\n",
      "80/80 [==============================] - 0s 376us/step - loss: 0.0116 - acc: 0.9875 - val_loss: 0.1310 - val_acc: 0.8500\n",
      "Epoch 299/400\n",
      "80/80 [==============================] - 0s 351us/step - loss: 0.0113 - acc: 0.9875 - val_loss: 0.1305 - val_acc: 0.8500\n",
      "Epoch 300/400\n",
      "80/80 [==============================] - 0s 348us/step - loss: 0.0110 - acc: 0.9875 - val_loss: 0.1306 - val_acc: 0.8500\n",
      "Epoch 301/400\n",
      "80/80 [==============================] - 0s 375us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.1299 - val_acc: 0.8500\n",
      "Epoch 302/400\n",
      "80/80 [==============================] - 0s 248us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.1291 - val_acc: 0.8500\n",
      "Epoch 303/400\n",
      "80/80 [==============================] - 0s 394us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.1286 - val_acc: 0.8500\n",
      "Epoch 304/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 349us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.1281 - val_acc: 0.8500\n",
      "Epoch 305/400\n",
      "80/80 [==============================] - 0s 424us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.1283 - val_acc: 0.8500\n",
      "Epoch 306/400\n",
      "80/80 [==============================] - 0s 293us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.1283 - val_acc: 0.8500\n",
      "Epoch 307/400\n",
      "80/80 [==============================] - 0s 358us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.1270 - val_acc: 0.8500\n",
      "Epoch 308/400\n",
      "80/80 [==============================] - 0s 288us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.1263 - val_acc: 0.8500\n",
      "Epoch 309/400\n",
      "80/80 [==============================] - 0s 289us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.1268 - val_acc: 0.8500\n",
      "Epoch 310/400\n",
      "80/80 [==============================] - 0s 424us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.1259 - val_acc: 0.8500\n",
      "Epoch 311/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.1255 - val_acc: 0.8500\n",
      "Epoch 312/400\n",
      "80/80 [==============================] - 0s 325us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.1246 - val_acc: 0.8500\n",
      "Epoch 313/400\n",
      "80/80 [==============================] - 0s 378us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.1248 - val_acc: 0.8500\n",
      "Epoch 314/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.1244 - val_acc: 0.8500\n",
      "Epoch 315/400\n",
      "80/80 [==============================] - 0s 317us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.1237 - val_acc: 0.8500\n",
      "Epoch 316/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.1231 - val_acc: 0.8500\n",
      "Epoch 317/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.1227 - val_acc: 0.8500\n",
      "Epoch 318/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.1219 - val_acc: 0.8500\n",
      "Epoch 319/400\n",
      "80/80 [==============================] - 0s 293us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.8500\n",
      "Epoch 320/400\n",
      "80/80 [==============================] - 0s 336us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.1223 - val_acc: 0.8500\n",
      "Epoch 321/400\n",
      "80/80 [==============================] - 0s 333us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.1211 - val_acc: 0.8500\n",
      "Epoch 322/400\n",
      "80/80 [==============================] - 0s 282us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.1206 - val_acc: 0.8500\n",
      "Epoch 323/400\n",
      "80/80 [==============================] - 0s 297us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.1202 - val_acc: 0.8500\n",
      "Epoch 324/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.1193 - val_acc: 0.8500\n",
      "Epoch 325/400\n",
      "80/80 [==============================] - 0s 355us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.8500\n",
      "Epoch 326/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.1198 - val_acc: 0.8500\n",
      "Epoch 327/400\n",
      "80/80 [==============================] - 0s 315us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.8500\n",
      "Epoch 328/400\n",
      "80/80 [==============================] - 0s 382us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.1182 - val_acc: 0.8500\n",
      "Epoch 329/400\n",
      "80/80 [==============================] - 0s 279us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.1178 - val_acc: 0.8500\n",
      "Epoch 330/400\n",
      "80/80 [==============================] - 0s 360us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1169 - val_acc: 0.8500\n",
      "Epoch 331/400\n",
      "80/80 [==============================] - 0s 359us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.1169 - val_acc: 0.8500\n",
      "Epoch 332/400\n",
      "80/80 [==============================] - 0s 292us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9000\n",
      "Epoch 333/400\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.1164 - val_acc: 0.9000\n",
      "Epoch 334/400\n",
      "80/80 [==============================] - 0s 307us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9000\n",
      "Epoch 335/400\n",
      "80/80 [==============================] - 0s 452us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 0.9000\n",
      "Epoch 336/400\n",
      "80/80 [==============================] - 0s 365us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9000\n",
      "Epoch 337/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9000\n",
      "Epoch 338/400\n",
      "80/80 [==============================] - 0s 387us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.1154 - val_acc: 0.9000\n",
      "Epoch 339/400\n",
      "80/80 [==============================] - 0s 371us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 0.9000\n",
      "Epoch 340/400\n",
      "80/80 [==============================] - 0s 406us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.1142 - val_acc: 0.9000\n",
      "Epoch 341/400\n",
      "80/80 [==============================] - 0s 428us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9000\n",
      "Epoch 342/400\n",
      "80/80 [==============================] - 0s 367us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.1129 - val_acc: 0.9000\n",
      "Epoch 343/400\n",
      "80/80 [==============================] - 0s 393us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1139 - val_acc: 0.9000\n",
      "Epoch 344/400\n",
      "80/80 [==============================] - 0s 390us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.1137 - val_acc: 0.9000\n",
      "Epoch 345/400\n",
      "80/80 [==============================] - 0s 393us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.9000\n",
      "Epoch 346/400\n",
      "80/80 [==============================] - 0s 469us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 0.9000\n",
      "Epoch 347/400\n",
      "80/80 [==============================] - 0s 402us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.9000\n",
      "Epoch 348/400\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.1114 - val_acc: 0.9000\n",
      "Epoch 349/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1128 - val_acc: 0.9000\n",
      "Epoch 350/400\n",
      "80/80 [==============================] - 0s 484us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.1125 - val_acc: 0.9000\n",
      "Epoch 351/400\n",
      "80/80 [==============================] - 0s 305us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.1129 - val_acc: 0.9000\n",
      "Epoch 352/400\n",
      "80/80 [==============================] - 0s 282us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.1116 - val_acc: 0.9000\n",
      "Epoch 353/400\n",
      "80/80 [==============================] - 0s 426us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.1105 - val_acc: 0.9000\n",
      "Epoch 354/400\n",
      "80/80 [==============================] - 0s 357us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 0.9000\n",
      "Epoch 355/400\n",
      "80/80 [==============================] - 0s 379us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 0.9000\n",
      "Epoch 356/400\n",
      "80/80 [==============================] - 0s 347us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 0.9000\n",
      "Epoch 357/400\n",
      "80/80 [==============================] - 0s 346us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1103 - val_acc: 0.9000\n",
      "Epoch 358/400\n",
      "80/80 [==============================] - 0s 367us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1097 - val_acc: 0.9000\n",
      "Epoch 359/400\n",
      "80/80 [==============================] - 0s 344us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1101 - val_acc: 0.9000\n",
      "Epoch 360/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1093 - val_acc: 0.9000\n",
      "Epoch 361/400\n",
      "80/80 [==============================] - 0s 426us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1099 - val_acc: 0.9000\n",
      "Epoch 362/400\n",
      "80/80 [==============================] - 0s 438us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.1089 - val_acc: 0.9000\n",
      "Epoch 363/400\n",
      "80/80 [==============================] - 0s 327us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1089 - val_acc: 0.9000\n",
      "Epoch 364/400\n",
      "80/80 [==============================] - 0s 391us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1086 - val_acc: 0.9000\n",
      "Epoch 366/400\n",
      "80/80 [==============================] - 0s 303us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1087 - val_acc: 0.9000\n",
      "Epoch 367/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1085 - val_acc: 0.9000\n",
      "Epoch 368/400\n",
      "80/80 [==============================] - 0s 329us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1083 - val_acc: 0.9000\n",
      "Epoch 369/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1087 - val_acc: 0.9000\n",
      "Epoch 370/400\n",
      "80/80 [==============================] - 0s 268us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1085 - val_acc: 0.9000\n",
      "Epoch 371/400\n",
      "80/80 [==============================] - 0s 321us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9000\n",
      "Epoch 372/400\n",
      "80/80 [==============================] - 0s 383us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1085 - val_acc: 0.9000\n",
      "Epoch 373/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1079 - val_acc: 0.9000\n",
      "Epoch 374/400\n",
      "80/80 [==============================] - 0s 271us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 0.9000\n",
      "Epoch 375/400\n",
      "80/80 [==============================] - 0s 429us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1078 - val_acc: 0.9000\n",
      "Epoch 376/400\n",
      "80/80 [==============================] - 0s 305us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1078 - val_acc: 0.9000\n",
      "Epoch 377/400\n",
      "80/80 [==============================] - 0s 304us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9000\n",
      "Epoch 378/400\n",
      "80/80 [==============================] - 0s 373us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 0.9000\n",
      "Epoch 379/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 0.9000\n",
      "Epoch 380/400\n",
      "80/80 [==============================] - 0s 432us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9000\n",
      "Epoch 381/400\n",
      "80/80 [==============================] - 0s 295us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9000\n",
      "Epoch 382/400\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9000\n",
      "Epoch 383/400\n",
      "80/80 [==============================] - 0s 406us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9000\n",
      "Epoch 384/400\n",
      "80/80 [==============================] - 0s 251us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9000\n",
      "Epoch 385/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0033 - acc: 1.000 - 0s 310us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9000\n",
      "Epoch 386/400\n",
      "80/80 [==============================] - 0s 440us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9000\n",
      "Epoch 387/400\n",
      "80/80 [==============================] - 0s 267us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9000\n",
      "Epoch 388/400\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9000\n",
      "Epoch 389/400\n",
      "80/80 [==============================] - 0s 276us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9000\n",
      "Epoch 390/400\n",
      "80/80 [==============================] - 0s 296us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9000\n",
      "Epoch 391/400\n",
      "80/80 [==============================] - 0s 351us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1078 - val_acc: 0.9000\n",
      "Epoch 392/400\n",
      "80/80 [==============================] - 0s 269us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 0.9000\n",
      "Epoch 393/400\n",
      "80/80 [==============================] - 0s 298us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1065 - val_acc: 0.9000\n",
      "Epoch 394/400\n",
      "80/80 [==============================] - 0s 338us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9000\n",
      "Epoch 395/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 0.9000\n",
      "Epoch 396/400\n",
      "80/80 [==============================] - 0s 332us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1065 - val_acc: 0.9000\n",
      "Epoch 397/400\n",
      "80/80 [==============================] - 0s 273us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1066 - val_acc: 0.9000\n",
      "Epoch 398/400\n",
      "80/80 [==============================] - 0s 343us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1072 - val_acc: 0.9000\n",
      "Epoch 399/400\n",
      "80/80 [==============================] - 0s 391us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1066 - val_acc: 0.9000\n",
      "Epoch 400/400\n",
      "80/80 [==============================] - 0s 368us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=400, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEh9JREFUeJzt3W+MZXV9x/H3d/8lHbWg7NTi/pnBhNpuo1K4RWqoxdDq\nQipbm6YBJ0WtyWQTMfKgqdtsojRm09jG+qeidGoJ2p2IaURdDRSr0frAYpg1sMuK6IrssivCog22\nnQe45dsH9yxchrkzd+49954Zfu9XcnPv+Z3fued7f3PmM+eec+6dyEwkSc9/65ouQJI0Gga+JBXC\nwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAbmlrx5s2bc3JysqnVS9KadPDgwcczc7yf\nZRsL/MnJSebm5ppavSStSRFxrN9lPaQjSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1Ihlg38\niLg5Ih6LiPu6zI+I+GhEHI2IQxFxYf1lrj6zh2eZ/PAk6/56HZMfnmT28GzTJa0Jjlv/HDsNqpcP\nXt0CfAz4dJf5VwDnV7fXAJ+o7p+3Zg/PMv2laeZ/MQ/AsSeOMf2laQCmXjnVZGmrmuPWP8dOdVh2\nDz8zvwn8bIkuu4BPZ9tdwNkRcW5dBa5Ge7+29+lfvDPmfzHP3q/tbaiitcFx659jpzrUcQx/C/Bw\nx/SJqu05ImI6IuYiYu7UqVM1rLoZx584vqJ2tTlu/XPsVIeRnrTNzJnMbGVma3y8r+/+WRW2n7V9\nRe1qc9z659ipDnUE/klgW8f01qrteWvf5fsY2zj2rLaxjWPsu3xfQxWtDY5b/xw71aGOwD8AXFtd\nrXMJ8ERmPlLD865aU6+cYuZNM0ycNUEQTJw1wcybZjx5tgzHrX+OneoQmbl0h4jPAJcBm4FHgfcB\nGwEy86aICNpX8ewE5oG3Z+ay33vcarXSr0eWpJWJiIOZ2epn2WUvy8zMa5aZn8A7+1m5JGl0/KSt\nJBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtS\nIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXC\nwJekQhj4klQIA1+SCmHgS1Ihegr8iNgZEQ9ExNGI2LPI/LMi4ksRcW9EHImIt9dfqiRpEMsGfkSs\nB24ErgB2ANdExI4F3d4JfDczXw1cBnwwIjbVXKskaQC97OFfDBzNzAcz80ngVmDXgj4JvCgiAngh\n8DPgdK2VSpIG0kvgbwEe7pg+UbV1+hjwG8CPgcPAuzPzqVoqlCTVoq6Ttm8E7gFeBlwAfCwifnlh\np4iYjoi5iJg7depUTauWJPWil8A/CWzrmN5atXV6O3Bbth0FfgT8+sInysyZzGxlZmt8fLzfmiVJ\nfegl8O8Gzo+I86oTsVcDBxb0OQ5cDhARLwVeATxYZ6GSpMFsWK5DZp6OiOuAO4H1wM2ZeSQidlfz\nbwLeD9wSEYeBAN6TmY8PsW5J0gotG/gAmXk7cPuCtps6Hv8YeEO9pUmS6uQnbSWpEAa+JBXCwJek\nQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqE\ngS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfI3W7CxMTsK6de372dmmK5KKsaHpAlSQ\n2VmYnob5+fb0sWPtaYCpqebqkgrhHr5GZ+/eZ8L+jPn5drukoTPwNTrHj6+sXVKtDHyNzvbtK2uX\nVCsDX6Ozbx+MjT27bWys3S5p6Ax8jc7UFMzMwMQERLTvZ2Y8YSuNiFfpaLSmpgx4qSE97eFHxM6I\neCAijkbEni59LouIeyLiSET8R71lSpIGtewefkSsB24E/gA4AdwdEQcy87sdfc4GPg7szMzjEfEr\nwypYktSfXvbwLwaOZuaDmfkkcCuwa0GftwC3ZeZxgMx8rN4yJUmD6iXwtwAPd0yfqNo6/Rrw4oj4\nRkQcjIhr6ypQklSPuk7abgAuAi4Hfgn4z4i4KzO/39kpIqaBaYDtXnstSSPVyx7+SWBbx/TWqq3T\nCeDOzPzfzHwc+Cbw6oVPlJkzmdnKzNb4+Hi/NUuS+tBL4N8NnB8R50XEJuBq4MCCPl8ELo2IDREx\nBrwGuL/eUiVJg1j2kE5mno6I64A7gfXAzZl5JCJ2V/Nvysz7I+LfgEPAU8AnM/O+YRYuSVqZyMxG\nVtxqtXJubq6RdUvSWhURBzOz1c+yfrWCJBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAl\nqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IK\nYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKkRPgR8ROyPigYg4\nGhF7luj32xFxOiL+pL4SJUl1WDbwI2I9cCNwBbADuCYidnTp9wHgK3UXKUkaXC97+BcDRzPzwcx8\nErgV2LVIv3cBnwMeq7E+SVJNegn8LcDDHdMnqranRcQW4M3AJ5Z6ooiYjoi5iJg7derUSmuVJA2g\nrpO2Hwbek5lPLdUpM2cys5WZrfHx8ZpWLUnqxYYe+pwEtnVMb63aOrWAWyMCYDNwZUSczswv1FKl\nJGlgvQT+3cD5EXEe7aC/GnhLZ4fMPO/M44i4BfiyYS9Jq8uygZ+ZpyPiOuBOYD1wc2YeiYjd1fyb\nhlyjJKkGvezhk5m3A7cvaFs06DPzbYOXJUmqm5+0laRCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw\n8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANf\nkgph4EtSIQx8SSqEgS9JhTDw+zU7C5OTsG5d+352tumKJGlJG5ouYE2anYXpaZifb08fO9aeBpia\naq4uSVqCe/j92Lv3mbA/Y36+3S5Jq5SB34/jx1fWLkmrgIHfj+3bV9YuSauAgd+PfftgbOzZbWNj\n7XZJWqUM/H5MTcHMDExMQET7fmbGE7aSVrWeAj8idkbEAxFxNCL2LDJ/KiIORcThiPhWRLy6/lJX\nmakpeOgheOqp9r1hL2mVWzbwI2I9cCNwBbADuCYidizo9iPg9zLzlcD7gZm6C5UkDaaXPfyLgaOZ\n+WBmPgncCuzq7JCZ38rM/6om7wK21lumJGlQvQT+FuDhjukTVVs37wDuGKQoSVL9av2kbUS8nnbg\nX9pl/jQwDbDdSxglaaR62cM/CWzrmN5atT1LRLwK+CSwKzN/utgTZeZMZrYyszU+Pt5PvZKkPvUS\n+HcD50fEeRGxCbgaONDZISK2A7cBf5aZ36+/TEnSoJY9pJOZpyPiOuBOYD1wc2YeiYjd1fybgPcC\n5wAfjwiA05nZGl7ZkqSVisxsZMWtVivn5uYaWbckrVURcbDfHWo/aStJhTDwJakQBr4kFcLAl6RC\nGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSB\nL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiS\nVAgDX5IK0VPgR8TOiHggIo5GxJ5F5kdEfLSafygiLqy/1AVmZ2FyEtata9/Pzo52+UE0XXvTyw+i\nydqbHjdfe3m11y0zl7wB64EfAi8HNgH3AjsW9LkSuAMI4BLg28s970UXXZR9278/91+0MSeuJ+N9\n5MT15P6LNmbu3z+a5QfRdO1NLz+IJmtvetz2788cG8uEZ25jY8W89iJr7wKYy2Xytdst2st3FxG/\nA9yQmW+spv+q+kPxNx19/hH4RmZ+ppp+ALgsMx/p9rytVivn5ub6+RvF7Os3M/3anzK/6Zm2sSdh\n5lvnMPX1x4e+/CCarr3p5QfRZO2Nj9vkJBw79tz2iQl46KGhrr/p115q7d1ExMHMbPWzbC+HdLYA\nD3dMn6jaVtqnNnsvePYAAsxvarePYvlBNF1708sPosnaGx+348dX1l7j+pt+7aXWPgwjPWkbEdMR\nMRcRc6dOner7eY6ftbL2upcfRNO1N738IJqsvfFx2759Ze01rr/p115q7cPQS+CfBLZ1TG+t2lba\nh8ycycxWZrbGx8dXWuvTtm88Z0XtdS8/iKZrb3r5QTRZe+Pjtm8fjI09u21srN0+5PU3/dpLrX0Y\negn8u4HzI+K8iNgEXA0cWNDnAHBtdbXOJcATSx2/H9S+qz7CWDz7fdJYbGLfVR8ZyfKDaLr2ppcf\nRJO1Nz5uU1MwM9M+Zh/Rvp+ZabcPef1Nv/ZSax+KXs7s0r4K5/u0r9bZW7XtBnZXjwO4sZp/GGgt\n95wDXaWTmfsP7c+JD01k3BA58aGJ3H9oZWe9B11+EE3X3vTyg2iy9rU8boOuv+nXXmrti2GYV+kM\nyyBX6UhSqYZ9lY4k6XnAwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFaOw6/Ig4BSzy9X8rthkY\n7tc0DmY112dt/VnNtcHqrs/a+nemvonM7Ou7aRoL/LpExFy/H0IYhdVcn7X1ZzXXBqu7PmvrXx31\neUhHkgph4EtSIZ4PgT/TdAHLWM31WVt/VnNtsLrrs7b+DVzfmj+GL0nqzfNhD1+S1IM1E/gRsTMi\nHoiIoxGxZ5H5EREfreYfiogLR1TXtoj4ekR8NyKORMS7F+lzWUQ8ERH3VLf3jqK2jvU/FBGHq3U/\n5zupGxy7V3SMyT0R8fOIuH5Bn5GNXUTcHBGPRcR9HW0viYh/j4gfVPcv7rLsktvnEOv7u4j4XvVz\n+3xEnN1l2SW3gSHVdkNEnOz42V3ZZdmhjl2X2j7bUddDEXFPl2WHPW6L5sfQtrt+v0h/lDdgPe1/\nrvJyYBNwL7BjQZ8rgTto/zOWS4Bvj6i2c4ELq8cvov2PYhbWdhnw5QbH7yFg8xLzGxm7RX7GP6F9\njXEjYwe8DrgQuK+j7W+BPdXjPcAHutS+5PY5xPreAGyoHn9gsfp62QaGVNsNwF/08HMf6tgtVtuC\n+R8E3tvQuC2aH8Pa7tbKHv7FwNHMfDAznwRuBXYt6LML+HS23QWcHRHnDruwzHwkM79TPf5v4H5g\ny7DXW7NGxm6By4EfZmYdH8brS2Z+E/jZguZdwKeqx58C/miRRXvZPodSX2Z+JTNPV5N30f5/0iPX\nZex6MfSxW6q2iAjgT4HP1LnOXi2RH0PZ7tZK4G8BHu6YPsFzQ7WXPkMVEZPAbwHfXmT2a6u33XdE\nxG+Osi4gga9GxMGImF5kfuNjR/t/JXf7pWty7F6az/x/5p8AL12kz2oYP4A/p/1ObTHLbQPD8q7q\nZ3dzl8MSTY/d7wKPZuYPuswf2bgtyI+hbHdrJfBXvYh4IfA54PrM/PmC2d8Btmfmq4B/AL4w4vIu\nzcwLgCuAd0bE60a8/iVFxCbgKuBfF5nd9Ng9Ldvvo1flZW0RsRc4Dcx26dLENvAJ2ocbLgAeoX3o\nZLW5hqX37kcybkvlR53b3VoJ/JPAto7prVXbSvsMRURspP3Dms3M2xbOz8yfZ+b/VI9vBzZGxOZR\n1Fat82R1/xjwedpvBTs1NnaVK4DvZOajC2c0PXbAo2cOb1X3jy3Sp9Hxi4i3AX8ITFXh8Bw9bAO1\ny8xHM/P/MvMp4J+6rLPJ39sNwB8Dn+3WZxTj1iU/hrLdrZXAvxs4PyLOq/YGrwYOLOhzALi2uuLk\nEuCJjrdEQ1MdA/xn4P7M/PsufX616kdEXEx73H867Nqq9b0gIl505jHtk3z3LejWyNh16LqX1eTY\nVQ4Ab60evxX44iJ9etk+hyIidgJ/CVyVmfNd+vSyDQyjts7zQG/uss7Gxg74feB7mXlisZmjGLcl\n8mM4292wzj4P4Wz2lbTPYP8Q2Fu17QZ2V48DuLGafxhojaiuS2m/3ToE3FPdrlxQ23XAEdpn0e8C\nXjvCcXt5td57qxpWzdhV634B7QA/q6OtkbGj/UfnEeAXtI+HvgM4B/ga8APgq8BLqr4vA25favsc\nUX1HaR/HPbPt3bSwvm7bwAhq+5dqezpEO4jObWLsFqutar/lzHbW0XfU49YtP4ay3flJW0kqxFo5\npCNJGpCBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIf4fXsjudnjvl8oAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff487196950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20),results, c='r')\n",
    "plt.scatter(range(20),y_test, c = 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB8l7hFJLgsGzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4864f9a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
